<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Speech to Text</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        background-color: #f4f4f9;
        color: #333;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        height: 100vh;
        margin: 0;
      }

      h1 {
        color: #CA2B26;
        margin-bottom: 20px;
      }

      button {
        background-color: #CA2B26;
        color: white;
        border: none;
        padding: 10px 20px;
        margin: 5px;
        border-radius: 5px;
        cursor: pointer;
        font-size: 16px;
      }

      button:hover {
        background-color: #357abd;
      }

      #output {
        margin-top: 20px;
        padding: 10px;
        width: 80%;
        height: 150px; /* Set a fixed height */
        background-color: #fff;
        border: 1px solid #ccc;
        border-radius: 5px;
        overflow-y: auto; /* Enable vertical scrolling */
        white-space: pre-wrap;
        word-wrap: break-word;
        text-align: left;
        font-size: 16px;
        color: #555;
      }

      .hidden {
        display: none;
      }
    </style>
  </head>
  <body>
    <h1>Speech to Text</h1>
    <button id="start-btn">Start</button>
    <button id="stop-btn" class="hidden">Stop</button>
    <div id="output">The transcript will appear here...</div>

    <script>
      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();

      const output = document.querySelector("#output");
      const startBtn = document.querySelector("#start-btn");
      const stopBtn = document.querySelector("#stop-btn");

      recognition.continuous = true;
      let isRecording = false;
      let isFirstTranscript = true; // To check if we need to replace the initial text

      // Start recording
      startBtn.addEventListener("click", () => {
        recognition.start();
        isRecording = true;
        startBtn.classList.add("hidden");
        stopBtn.classList.remove("hidden");
      });

      // Stop recording
      stopBtn.addEventListener("click", () => {
        recognition.stop();
        isRecording = false;
        stopBtn.classList.add("hidden");
        startBtn.classList.remove("hidden");
      });

      // Event for handling the results
      recognition.addEventListener("result", (event) => {
        // console.log(event.results[event.results.length - 1])
        // console.log(event.results[event.results.length - 1].SpeechRecognitionAlternative)
        const transcript = event.results[event.results.length - 1][0].transcript
        // Replace initial text on the first transcript
        if (isFirstTranscript) {
          output.textContent = "";
          isFirstTranscript = false;
        }

        // Speech 1 |2   |3     |4
        // Words  a |b   |c     |d
        // Result a |a b |a b c |a b c d
        // Append new transcript to the existing text without repeating
        output.textContent += ` ${transcript}`;

        // Auto-scroll to the bottom of the output box
        output.scrollTop = output.scrollHeight;

        // Send transcript via POST API call
        sendTranscript(output.textContent);
      });

      // Restart recognition if it stops unexpectedly
      recognition.addEventListener("end", () => {
        if (isRecording) {
          recognition.start();
        }
      });

      // Function to send transcript via API call
      const sendTranscript = async (transcript) => {
        const data = {
          timestamp: new Date().toISOString(),
          roomName: "qline", // Updated roomName
          description: "qline Accessibility Stream",
          transcript: transcript,
          isActivelyStreaming: true,
          translation: "",
          inputLanguage: "en",
          outputLanguage: "ar",
          isPremiumCustomer: true,
        };

        try {
          const response = await fetch(
            "https://api.deafassistant.com/stream/LiteSmartAdd",
            {
              method: "POST",
              headers: {
                "Content-Type": "application/json",
              },
              body: JSON.stringify(data),
            }
          );

          if (!response.ok) {
            throw new Error("Failed to send transcript");
          }

          console.log("Transcript sent successfully");
        } catch (error) {
          console.error(error.message);
        }
      };

      // Check every second to ensure recording continues if the user taps "Start"
    //   setInterval(() => {
    //     if (isRecording && !recognition.starting) {
    //       recognition.start(); // Keep starting recognition to avoid it stopping unexpectedly
    //     }
    //   }, 1000);
    </script>
  </body>
</html>
